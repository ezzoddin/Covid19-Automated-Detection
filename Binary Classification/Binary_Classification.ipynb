{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lyjBE4vWF-It"
   },
   "outputs": [],
   "source": [
    "# filter warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# keras imports\n",
    "from keras.applications.densenet import DenseNet169 , preprocess_input\n",
    "\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# other imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rFLJPBmf0k9"
   },
   "source": [
    "# load the user configs , from config.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UBSE_mltGSSF"
   },
   "outputs": [],
   "source": [
    "# load the user configs\n",
    "\n",
    "# use your own /config/config.json path\n",
    "with open('/content/drive/MyDrive/covid19/config/config.json') as f:    \n",
    "  config = json.load(f)\n",
    "\n",
    "\n",
    "# config variables\n",
    "model_name      = config[\"model\"]\n",
    "weights         = config[\"weights\"]\n",
    "include_top     = config[\"include_top\"]\n",
    "train_path      = config[\"train_path\"]\n",
    "features_path   = config[\"features_path\"]\n",
    "labels_path     = config[\"labels_path\"]\n",
    "test_size       = config[\"test_size\"]\n",
    "model_path      = config[\"model_path\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lt1ptSNgguoK"
   },
   "source": [
    "# Use DenseNet169 as Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s88s3PAkgvjs",
    "outputId": "f42b17fa-426e-46ff-824d-1304d387ac35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] successfully loaded base model and model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DenseNet169 as model \n",
    "\n",
    "if model_name == \"DenseNet169\":\n",
    "  base_model = DenseNet169(include_top=include_top, weights=weights, input_tensor=Input(shape=(224,224,3)), input_shape=(224,224,3), pooling=\"avg\")\n",
    "  model = Model(base_model.input, base_model.get_layer('avg_pool').output)\n",
    "  image_size = (224, 224)\n",
    "\n",
    "else:\n",
    "  base_mode = none\n",
    "\n",
    "\n",
    "print(\"[INFO] successfully loaded base model and model...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4klaW76hgQf"
   },
   "source": [
    "# Exteract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvHff4FJoLeS"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# path to training dataset\n",
    "train_labels = os.listdir(train_path)\n",
    "\n",
    "# encode the labels\n",
    "print(\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "le.fit([tl for tl in train_labels])\n",
    "\n",
    "# variables to hold features and labels\n",
    "features = []\n",
    "labels   = []\n",
    "\n",
    "\n",
    "\n",
    "# loop over all the labels in the folder\n",
    "count = 1\n",
    "for i, label in enumerate(train_labels):\n",
    "  cur_path = train_path + \"/\" + label\n",
    "  count = 1\n",
    "  for image_path in glob.glob(cur_path + \"/*\"):\n",
    "    img = image.load_img(image_path, target_size=image_size)\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    feature = model.predict(x)\n",
    "    flat = feature.flatten()\n",
    "    features.append(flat)\n",
    "    labels.append(label)\n",
    "    print(\"[INFO] processed - \" + str(count))\n",
    "    count += 1\n",
    "  print(\"[INFO] completed label - \" + label)\n",
    "\n",
    "# encode the labels using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le_labels = le.fit_transform(labels)\n",
    "\n",
    "# get the shape of training labels\n",
    "print(\"[STATUS] training labels: {}\".format(le_labels))\n",
    "print(\"[STATUS] training labels shape: {}\".format(le_labels.shape))\n",
    "\n",
    "# save features and labels\n",
    "h5f_data = h5py.File(features_path, 'w')\n",
    "h5f_data.create_dataset('dataset_1', data=np.array(features))\n",
    "\n",
    "h5f_label = h5py.File(labels_path, 'w')\n",
    "h5f_label.create_dataset('dataset_1', data=np.array(le_labels))\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "# save model and weights\n",
    "model_json = model.to_json()\n",
    "with open(model_path + str(test_size) + \".json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "\n",
    "# save weights\n",
    "model.save_weights(model_path + str(test_size) + \".h5\")\n",
    "print(\"[STATUS] saved model and weights to disk..\")\n",
    "\n",
    "print(\"[STATUS] features and labels saved..\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIja04Hj87TS"
   },
   "outputs": [],
   "source": [
    "# load the user configs\n",
    "# use your own /config/config.json path\n",
    "with open('/content/drive/MyDrive/covid19/config/config.json') as f:    \n",
    "  config = json.load(f)\n",
    "\n",
    "# config variables\n",
    "test_size        = config[\"test_size\"]\n",
    "seed             = config[\"seed\"]\n",
    "features_path    = config[\"features_path\"]\n",
    "labels_path      = config[\"labels_path\"]\n",
    "results          = config[\"results\"]\n",
    "classifier_path  = config[\"classifier_path\"]\n",
    "train_path       = config[\"train_path\"]\n",
    "num_classes      = config[\"num_classes\"]\n",
    "classifier_path  = config[\"classifier_path\"]\n",
    "selection = config[\"selection\"]\n",
    "# import features and labels\n",
    "h5f_data  = h5py.File(features_path, 'r')\n",
    "h5f_label = h5py.File(labels_path, 'r')\n",
    "\n",
    "features_string = h5f_data['dataset_1']\n",
    "labels_string   = h5f_label['dataset_1']\n",
    "\n",
    "features = np.array(features_string)\n",
    "labels   = np.array(labels_string)\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "scoring = {\n",
    "    'accuracy'    : make_scorer(metrics.accuracy_score),\n",
    "    'precision'   : make_scorer(metrics.precision_score),\n",
    "    'sensitivity' : make_scorer(metrics.recall_score),\n",
    "    'specificity' : make_scorer(metrics.recall_score,pos_label=0)\n",
    "}\n",
    "\n",
    "\n",
    "n = int(features.shape[1]*8/100)\n",
    "\n",
    "rank_1 = 0\n",
    "fs = SelectKBest(score_func=f_classif, k=n)\n",
    "X_selected = fs.fit_transform(features, labels)\n",
    "\n",
    "print(\"[INFO] features shape: {}\".format(X_selected.shape))\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=5, random_state=7803, shuffle=True)\n",
    "\n",
    "# create model\n",
    "model = LGBMClassifier(learning_rate=0.2)\n",
    "\n",
    "# evaluate model\n",
    "scores_accuracy = cross_val_score(model, X_selected, labels, scoring=scoring['accuracy'], cv=cv, n_jobs=-1)\n",
    "scores_accuracy  = [ elem*100 for elem in scores_accuracy ]\n",
    "print(scores_accuracy)\n",
    "print(' Accuracy: %.2f' % (mean(scores_accuracy)))\n",
    "\n",
    "\n",
    "print(\"\\n***\")\n",
    "scores_f1_score = cross_val_score(model, X_selected, labels, scoring= 'f1_macro' , cv=cv, n_jobs=-1)\n",
    "scores_f1_score  = [ elem*100 for elem in scores_f1_score ]\n",
    "print(scores_f1_score)\n",
    "print( ' f1_score: %.2f' % (mean(scores_f1_score)))\n",
    "\n",
    "print(\"\\n***\")\n",
    "scores_precision = cross_val_score(model, X_selected, labels, scoring=scoring['precision'], cv=cv, n_jobs=-1)\n",
    "scores_precision  = [ elem*100 for elem in scores_precision ]\n",
    "print(scores_precision)\n",
    "print( ' precision: %.2f' % (mean(scores_precision)))\n",
    "\n",
    "\n",
    "print(\"\\n***\")\n",
    "scores_specificity = cross_val_score(model, X_selected, labels, scoring=scoring['specificity'], cv=cv, n_jobs=-1)\n",
    "scores_specificity  = [ elem*100 for elem in scores_specificity ]\n",
    "print(scores_specificity)\n",
    "print( ' specificity: %.2f' % (mean(scores_specificity)))\n",
    "\n",
    "\n",
    "print(\"\\n***\")\n",
    "scores_sensitivity = cross_val_score(model, X_selected, labels, scoring=scoring['sensitivity'], cv=cv, n_jobs=-1)\n",
    "scores_sensitivity  = [ elem*100 for elem in scores_sensitivity ]\n",
    "print(scores_sensitivity)\n",
    "print( ' sensitivity: %.2f' % (mean(scores_sensitivity)))\n",
    "\n",
    "\n",
    "y_pred = cross_val_predict(model, X_selected, labels , cv=cv)\n",
    "cm = confusion_matrix( labels , y_pred)\n",
    "labels_matrix = sorted(list(os.listdir(train_path)))\n",
    "\n",
    "\n",
    "print(cm)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.set(font_scale=2) # Adjust to fit\n",
    "\n",
    "print(\"================\")\n",
    "\n",
    "\n",
    "sns.heatmap(cm,\n",
    "          yticklabels = labels_matrix,\n",
    "          xticklabels = labels_matrix,\n",
    "          annot=True,\n",
    "          cmap='binary',\n",
    "          fmt=\".0f\")\n",
    "\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)  # Adjust to fit\n",
    "\n",
    "\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "image_name = \"/content/drive/MyDrive/covid19/All_data.svg\"\n",
    "\n",
    "\n",
    "plt.title(\"All Data\")\n",
    "fig.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confision matrix for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NE3b_nFgsxD-"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "f = 1\n",
    "result = {}\n",
    "labels_matrix = sorted(list(os.listdir(train_path)))\n",
    "conf_matrix_list_of_arrays = []\n",
    "acc_a = []\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=7803, shuffle=True)\n",
    "\n",
    "for train_ix, test_ix in cv.split(X_selected):\n",
    "    # get data\n",
    "    train_X, test_X = X_selected[train_ix], X_selected[test_ix]\n",
    "    train_y, test_y = labels[train_ix], labels[test_ix]\n",
    "    # fit model\n",
    "    model = LGBMClassifier(learning_rate=0.2)\n",
    "    model.fit(train_X, train_y)\n",
    "    # evaluate model\n",
    "    yhat = model.predict(test_X)\n",
    "\n",
    "    cm = confusion_matrix(test_y, yhat)\n",
    "\n",
    "    labels_matrix = sorted(list(os.listdir(train_path)))\n",
    "\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.set(font_scale=2) # Adjust to fit\n",
    "\n",
    "\n",
    "    print(\"================\")\n",
    "\n",
    "    sns.heatmap(cm,\n",
    "              yticklabels = labels_matrix,\n",
    "              xticklabels = labels_matrix,\n",
    "              annot=True,\n",
    "              cmap='binary',\n",
    "              fmt=\".0f\")\n",
    "\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)  # Adjust to fit\n",
    "\n",
    "\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = \"/content/drive/MyDrive/covid19/output/confision_matrix_\" + str(f) + \".svg\"\n",
    "\n",
    "\n",
    "    plt.title(\"Fold \"+ str(f))\n",
    "    fig.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "    f +=1\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "covid19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
